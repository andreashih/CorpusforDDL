{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/CorpusforDDL_compute/corpus_compute')\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cozs-mhlJzVr",
        "outputId": "6a7f0f4d-301c-4a66-e016-590a133c100a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/CorpusforDDL_compute/corpus_compute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import pickle\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "NyKR_qdoJ2Gy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入事先寫好的 functions\n",
        "from compute_parameters import *"
      ],
      "metadata": {
        "id": "yoOkHW-9TKAH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus():\n",
        "  def __init__(self, ws, ws_pos, text):\n",
        "    self.ws = ws\n",
        "    self.ws_pos = ws_pos\n",
        "    self.text = text\n",
        "  \n",
        "  def sentence_length(self):\n",
        "    sentence_length = []\n",
        "    for text in self.ws:\n",
        "      length = [len(sentence) for sentence in text]\n",
        "      sentence_length.append(length)\n",
        "    return sentence_length\n",
        "  \n",
        "  def high_low_freq(self):\n",
        "    high_low_freq = []\n",
        "    for text in self.ws:\n",
        "      freq = [get_high_low_freq(sentence) for sentence in text]\n",
        "      high_low_freq.append(freq)\n",
        "    return high_low_freq\n",
        "  \n",
        "  def word_freq(self):\n",
        "    word_freq = []\n",
        "    for text in self.ws:\n",
        "      freq = [get_word_freq(sentence) for sentence in text]\n",
        "      word_freq.append(freq)\n",
        "    return word_freq\n",
        "  \n",
        "  def word_level(self):\n",
        "    word_level = []\n",
        "    for text in self.ws:\n",
        "      level = [get_word_level(sentence) for sentence in text]\n",
        "      word_level.append(level)\n",
        "    return word_level\n",
        "  \n",
        "  def long_word_count(self):\n",
        "    long_word_count = []\n",
        "    for text in self.ws:\n",
        "      count = [get_long_word_count(sentence) for sentence in text]\n",
        "      long_word_count.append(count)\n",
        "    return long_word_count\n",
        "  \n",
        "  def is_complete_sentence(self):\n",
        "    is_complete_sentence = []\n",
        "    for text in self.ws_pos:\n",
        "      sent = [get_complete_sentence(sentence) for sentence in text]\n",
        "      is_complete_sentence.append(sent)\n",
        "    return is_complete_sentence\n",
        "  \n",
        "  def is_complete_context(self):\n",
        "    is_complete_context = []\n",
        "    for text in self.ws_pos:\n",
        "      sent = [get_complete_context(sentence) for sentence in text]\n",
        "      is_complete_context.append(sent)\n",
        "    return is_complete_context\n",
        "  \n",
        "  def is_greylist(self):\n",
        "    is_greylist = []\n",
        "    for text in self.ws_pos:\n",
        "      gl = [get_greylist(x) for x in text]\n",
        "      is_greylist.append(gl)\n",
        "    return is_greylist\n",
        "  \n",
        "  def is_blacklist(self):\n",
        "    is_blacklist = []\n",
        "    for text in self.text:\n",
        "      bl = [get_blacklist(x) for x in text]\n",
        "      is_blacklist.append(bl)\n",
        "    return is_blacklist"
      ],
      "metadata": {
        "id": "GBcSkp3Tlw3a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import Text\n",
        "def make_concordance_df(target_words, ws):\n",
        "\n",
        "  corpus = [item for sublist in ws for item in sublist]\n",
        "  text = Text(corpus)\n",
        "  dfs = []\n",
        "\n",
        "  for word in target_words:\n",
        "\n",
        "    con_list = text.concordance_list(word)\n",
        "    right_word = [x.right[0] for x in con_list]\n",
        "    left_word = [x.left[-1] for x in con_list]\n",
        "    context = [x.left + [word] + x.right for x in con_list]\n",
        "    context = [' '.join(x) for x in context]\n",
        "\n",
        "    df = pd.DataFrame({'left_word': left_word,\n",
        "                      'target_word': word,\n",
        "                      'right_word': right_word,\n",
        "                      'context': context})\n",
        "    dfs.append(df)\n",
        "\n",
        "  return dfs"
      ],
      "metadata": {
        "id": "CtiqI399vqik"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ASBC"
      ],
      "metadata": {
        "id": "KpkoaJCnlsIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 讀入 ASBC txt 檔\n",
        "asbc_path = '../CorpusforDDL/ASBC_去XML標記'\n",
        "all_files = sorted(os.listdir(asbc_path))\n",
        "\n",
        "asbc_corpus = []\n",
        "for f in all_files:\n",
        "  with open(f'{asbc_path}/{f}') as f:\n",
        "      lines = f.readlines()\n",
        "      asbc_corpus.append(lines)"
      ],
      "metadata": {
        "id": "TZiI9XC1Jurx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前處理\n",
        "def preprocess_asbc(string):\n",
        "\n",
        "  clean_string = unicodedata.normalize('NFKC', string) # 全形轉半形\n",
        "  clean_string = re.sub(r'\\n', '', clean_string) # 移除換行符號\n",
        "  clean_string = re.sub(r'\\-+', '', clean_string) # 移除連續的 -\n",
        "  clean_string = re.sub(r'\\[\\+[A-z0-9]+\\]', '', clean_string) # 移除特徵標記\n",
        "  clean_string = re.sub(r'\\(\\w+CATEGORY\\)', '', clean_string) # 移除標點符號標記\n",
        "\n",
        "  clean_string = clean_string.translate(str.maketrans({',': '，', \n",
        "                                                       '!': '！', \n",
        "                                                       '?': '？',\n",
        "                                                       ':': '：',\n",
        "                                                       ';': '；'})) # 部分符號改回全形\n",
        "  return clean_string"
      ],
      "metadata": {
        "id": "R6Zw83EdMWop"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 語料格式準備 - 1\n",
        "斷詞 + pos tag"
      ],
      "metadata": {
        "id": "-1xGNb1buHdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_ws_pos = []\n",
        "\n",
        "for text in asbc_corpus:\n",
        "  preprocessed = [preprocess_asbc(string) for string in text]\n",
        "  joined = ''.join(preprocessed)\n",
        "  split_1 = re.split(r'(?<=。」|！」|？」)', joined) # 先用 。」 ！」 ？」 分隔\n",
        "  split_2 = [re.split(r'(?<=[。！？])(?!」)', x) for x in split_1] # 再用 。！？ 分隔\n",
        "  split = [item for sublist in split_2 for item in sublist] # 將 list of list 攤平\n",
        "  split = [x for x in split if len(x)>1]\n",
        "\n",
        "  asbc_ws_pos.append(split)"
      ],
      "metadata": {
        "id": "_1I0xgRLNpMG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_ws_pos[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vt2ougcbNs-7",
        "outputId": "d573530c-5cb6-4062-b24b-c16926b91673"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff時間(Na) ：三月(Nd) 十日(Nd) ( 星期四(Nd) ) 上午(Nd) 十時(Nd) 。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 語料格式準備 - 2\n",
        "斷好詞，無標點符號"
      ],
      "metadata": {
        "id": "mF8qf23CuP2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_ws = []\n",
        "for text in asbc_ws_pos:\n",
        "  ws = []\n",
        "  for sent in text:\n",
        "    sent_words = re.sub(r'\\([A-z0-9]+\\)', '', sent)\n",
        "    sent_words = re.sub(r'[^\\w\\s]', '', sent_words).strip().split(' ')\n",
        "    sent_words = list(filter(None, sent_words))\n",
        "    ws.append(sent_words)\n",
        "  asbc_ws.append(ws)"
      ],
      "metadata": {
        "id": "DMijYJOOl5C3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_ws[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXlz5Kx4Wu0M",
        "outputId": "41cdae15-9d82-4e09-96b5-90c280ae1bfb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['時間', '三月', '十日', '星期四', '上午', '十時']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 語料格式準備 - 3\n",
        "完整句子，含標點符號"
      ],
      "metadata": {
        "id": "QVSjq17muTE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_text = []\n",
        "\n",
        "for text in asbc_ws_pos:\n",
        "  t = []\n",
        "  for sent in text:\n",
        "    res = re.sub(r'\\([A-z0-9_]+\\)', '', sent)\n",
        "    res = res.replace(' ', '')\n",
        "    t.append(res)\n",
        "  asbc_text.append(t)"
      ],
      "metadata": {
        "id": "R590l9VDXFL0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_text[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gjuCoLaXXbDN",
        "outputId": "8c9f1928-d741-4d95-d6c7-92ff0b4b5867"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff時間：三月十日(星期四)上午十時。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 計算指標"
      ],
      "metadata": {
        "id": "8GTFYfv3vfBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_corpus = Corpus(asbc_ws, asbc_ws_pos, asbc_text)"
      ],
      "metadata": {
        "id": "J0B85fZFQIaW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_sentence_length = asbc_corpus.sentence_length()\n",
        "asbc_high_low_freq = asbc_corpus.high_low_freq()\n",
        "asbc_word_freq = asbc_corpus.word_freq()\n",
        "asbc_word_level = asbc_corpus.word_level()\n",
        "asbc_long_word_count = asbc_corpus.long_word_count()\n",
        "asbc_is_complete_sentence = asbc_corpus.is_complete_sentence()\n",
        "asbc_is_complete_context = asbc_corpus.is_complete_context()\n",
        "asbc_is_greylist = asbc_corpus.is_greylist()\n",
        "asbc_is_blacklist = asbc_corpus.is_blacklist()"
      ],
      "metadata": {
        "id": "J0arl51FYd3i"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 製作表格"
      ],
      "metadata": {
        "id": "QAJ8V1SyvhRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asbc_dfs = []\n",
        "\n",
        "for text, ws, length, w_freq, hl_freq, level, count, complete_sent, complete_cont, bl, gl in zip(asbc_text, asbc_ws, \n",
        "                  asbc_sentence_length, asbc_word_freq, asbc_high_low_freq,\n",
        "                  asbc_word_level, asbc_long_word_count, asbc_is_complete_sentence, \n",
        "                  asbc_is_complete_context, asbc_is_blacklist, asbc_is_greylist):\n",
        "  \n",
        "  ws = [' '.join(sent) for sent in ws]\n",
        "  asbc_df = pd.DataFrame({'sentence': text,\n",
        "                        'sentence_preprocessed': ws,\n",
        "                        'sentence_length': length,\n",
        "                        'word_freq': w_freq,\n",
        "                        'high_low_freq': hl_freq,\n",
        "                        'word_level': level,\n",
        "                        'long_word_count': count,\n",
        "                        'is_complete_sentence': complete_sent,\n",
        "                        'is_complete_context': complete_cont,\n",
        "                        'is_blacklist': bl,\n",
        "                        'is_greylist': gl})\n",
        "  asbc_df = asbc_df[asbc_df['sentence_length']>0]\n",
        "  asbc_dfs.append(asbc_df)"
      ],
      "metadata": {
        "id": "F2XnxKmvYGMj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename, df in zip(all_files, asbc_dfs):\n",
        "  filename = re.sub('.txt', '', filename)\n",
        "  df.to_csv(f'../results/asbc/asbc_parameters/parameters_{filename}.csv', index = False)"
      ],
      "metadata": {
        "id": "COgjcwEUwIaI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_words = ['難得', '畢竟', '的確', '難免', '總是', '有助於']\n",
        "\n",
        "asbc_concordance_dfs = []\n",
        "for text in asbc_ws:\n",
        "  dfs = make_concordance_df(target_words, text)\n",
        "  concordance_df = pd.concat(dfs)\n",
        "  asbc_concordance_dfs.append(concordance_df)"
      ],
      "metadata": {
        "id": "YM1NxqDfER9m"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename, df in zip(all_files, asbc_concordance_dfs):\n",
        "  filename = re.sub('.txt', '', filename)\n",
        "  df.to_csv(f'../results/asbc/asbc_concordance_df/concordance_df_{filename}.csv', index = False)"
      ],
      "metadata": {
        "id": "D4zqruYowObz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDsplit\n",
        "ram 不足，所以拆成兩份處理"
      ],
      "metadata": {
        "id": "esK1GrPQzCIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 讀入 EDsplit txt 檔\n",
        "edsplit_path = '../CorpusforDDL/EDsplit-1File1Days-DeleteRepeatPattern-TitleWithoutPeriod-out'\n",
        "all_files = sorted(os.listdir(edsplit_path))\n",
        "\n",
        "edsplit_corpus = []\n",
        "for f in all_files[:1000]:\n",
        "  with open(f'{edsplit_path}/{f}', 'rb') as f:\n",
        "      lines = f.readlines()\n",
        "      edsplit_corpus.append(lines)"
      ],
      "metadata": {
        "id": "7-7qW9kQDMyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86950699-d5be-4b68-b9b1-d698abe8b3b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.05 s, sys: 1.13 s, total: 2.17 s\n",
            "Wall time: 22.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_corpus_decoded = []\n",
        "\n",
        "for text in edsplit_corpus:\n",
        "  text_decoded = []\n",
        "  not_decoded = []\n",
        "  for string in text:\n",
        "    try:\n",
        "      text = string.decode('cp950')\n",
        "      text_decoded.append(text)\n",
        "    except:\n",
        "      not_decoded.append(string)\n",
        "  edsplit_corpus_decoded.append(text_decoded)"
      ],
      "metadata": {
        "id": "otOO_7dZFJZj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 語料格式準備 - 1\n",
        "斷詞 + pos tag"
      ],
      "metadata": {
        "id": "2uD03JQLXEAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 前處理\n",
        "def preprocess_edsplit(string):\n",
        "\n",
        "  clean_string = unicodedata.normalize('NFKC', string) # 全形轉半形\n",
        "  clean_string = re.sub(r'\\r|\\n', '', clean_string) # 移除換行符號\n",
        "  clean_string = re.sub(r'\\[\\+[A-z0-9]+\\]', '', clean_string) # 移除特徵標記\n",
        "  clean_string = re.sub(r'\\(\\w+CATEGORY\\)', '', clean_string) # 移除標點符號標記\n",
        "\n",
        "  clean_string = clean_string.translate(str.maketrans({',': '，', \n",
        "                                                       '!': '！', \n",
        "                                                       '?': '？'})) # ，！？改回全形\n",
        "  clean_string = clean_string.strip()\n",
        "  return clean_string"
      ],
      "metadata": {
        "id": "tax9Y2jJGTRu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_ws_pos = []\n",
        "\n",
        "for text in edsplit_corpus_decoded:\n",
        "  preprocessed = [preprocess_edsplit(string) for string in text]\n",
        "  joined = ''.join(preprocessed)\n",
        "  split_1 = re.split(r'(?<=。」|！」|？」)', joined) # 先用 。」 ！」 ？」 分隔\n",
        "  split_2 = [re.split(r'(?<=[。！？])(?!」)', x) for x in split_1] # 再用 。！？ 分隔\n",
        "  split = [item for sublist in split_2 for item in sublist] # 將 list of list 攤平\n",
        "\n",
        "  edsplit_ws_pos.append(split)"
      ],
      "metadata": {
        "id": "5ltzOJCF2C0J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_ws_pos[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sm6TphJe2Mgc",
        "outputId": "ee918f00-2f51-417f-995c-12a4bcdff346"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'信用卡(Na) 繳稅(VA) 今年(Nd) 將(D) 開辦(VC) 。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 語料格式準備 - 2\n",
        "斷好詞，無標點符號"
      ],
      "metadata": {
        "id": "Dwajf7ZZWaBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_ws = []\n",
        "for text in edsplit_ws_pos:\n",
        "  ws = []\n",
        "  for sent in text:\n",
        "    sent_words = re.sub(r'\\([A-z0-9]+\\)', '', sent)\n",
        "    sent_words = re.sub(r'[^\\w\\s]', '', sent_words).strip().split(' ')\n",
        "    sent_words = list(filter(None, sent_words))\n",
        "    ws.append(sent_words)\n",
        "  edsplit_ws.append(ws)"
      ],
      "metadata": {
        "id": "2Kt8zGv4ztpv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_ws[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpA1WCHnWk1F",
        "outputId": "96f1f8b3-974c-4b47-b942-63fe4491e220"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['信用卡', '繳稅', '今年', '將', '開辦']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 語料格式準備 - 3\n",
        "完整句子，含標點符號"
      ],
      "metadata": {
        "id": "Xkp7hgBZWwVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_text = []\n",
        "\n",
        "for text in edsplit_ws_pos:\n",
        "  t = []\n",
        "  for sent in text:\n",
        "    res = re.sub(r'\\([A-z0-9_]+\\)', '', sent)\n",
        "    res = res.replace(' ', '')\n",
        "    t.append(res)\n",
        "  edsplit_text.append(t)"
      ],
      "metadata": {
        "id": "7NRQHElYWoAE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_text[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Wr9Yli8izDXp",
        "outputId": "91ac8af8-9b62-4dbc-eb9a-3ce48c34d29e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'信用卡繳稅今年將開辦。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 計算指標"
      ],
      "metadata": {
        "id": "KaY10e5j0MoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_corpus = Corpus(edsplit_ws, edsplit_ws_pos, edsplit_text)"
      ],
      "metadata": {
        "id": "46IpnPec0MoU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_sentence_length = edsplit_corpus.sentence_length()\n",
        "edsplit_high_low_freq = edsplit_corpus.high_low_freq()\n",
        "edsplit_word_freq = edsplit_corpus.word_freq()\n",
        "edsplit_word_level = edsplit_corpus.word_level()\n",
        "edsplit_long_word_count = edsplit_corpus.long_word_count()\n",
        "edsplit_is_complete_sentence = edsplit_corpus.is_complete_sentence()\n",
        "edsplit_is_complete_context = edsplit_corpus.is_complete_context()\n",
        "edsplit_is_greylist = edsplit_corpus.is_greylist()\n",
        "edsplit_is_blacklist = edsplit_corpus.is_blacklist()"
      ],
      "metadata": {
        "id": "YnOHPUms0MoV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 製作表格"
      ],
      "metadata": {
        "id": "prbmdk1T0MoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edsplit_dfs = []\n",
        "\n",
        "for text, ws, length, w_freq, hl_freq, level, count, complete_sent, complete_cont, bl, gl in zip(edsplit_text, edsplit_ws, \n",
        "                  edsplit_sentence_length, edsplit_word_freq, edsplit_high_low_freq,\n",
        "                  edsplit_word_level, edsplit_long_word_count, edsplit_is_complete_sentence, \n",
        "                  edsplit_is_complete_context, edsplit_is_blacklist, edsplit_is_greylist):\n",
        "  \n",
        "  ws = [' '.join(sent) for sent in ws]\n",
        "  edsplit_df = pd.DataFrame({'sentence': text,\n",
        "                        'sentence_preprocessed': ws,\n",
        "                        'sentence_length': length,\n",
        "                        'word_freq': w_freq,\n",
        "                        'high_low_freq': hl_freq,\n",
        "                        'word_level': level,\n",
        "                        'long_word_count': count,\n",
        "                        'is_complete_sentence': complete_sent,\n",
        "                        'is_complete_context': complete_cont,\n",
        "                        'is_blacklist': bl,\n",
        "                        'is_greylist': gl})\n",
        "  edsplit_df = edsplit_df[edsplit_df['sentence_length']>0]\n",
        "  edsplit_dfs.append(edsplit_df)"
      ],
      "metadata": {
        "id": "rTS38ND20MoV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename, df in zip(all_files[:1000], edsplit_dfs):\n",
        "  filename = re.sub('.txt', '', filename)\n",
        "  df.to_csv(f'../results/EDsplit/EDsplit_parameters/parameters_{filename}.csv', index = False)"
      ],
      "metadata": {
        "id": "AbpSWAb40MoV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_words = ['難得', '畢竟', '的確', '難免', '總是', '有助於']\n",
        "\n",
        "edsplit_concordance_dfs = []\n",
        "for text in edsplit_ws:\n",
        "  dfs = make_concordance_df(target_words, text)\n",
        "  concordance_df = pd.concat(dfs)\n",
        "  edsplit_concordance_dfs.append(concordance_df)"
      ],
      "metadata": {
        "id": "d7r-NHYD0MoV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename, df in zip(all_files, edsplit_concordance_dfs):\n",
        "  filename = re.sub('.txt', '', filename)\n",
        "  df.to_csv(f'../results/EDsplit/EDsplit_concordance_df/concordance_df_{filename}.csv', index = False)"
      ],
      "metadata": {
        "id": "GyJTKmaC0MoV"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}